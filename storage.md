Storage

I. Block storage

    - like a hard disks attached your computer
    - one block storage device can be connected to one virtual server
II. File storage 

    - shared by several servers

ObJect storage:

    - object versioning: # prevent delete 
        live version:
        old version: auth generated by google
    
    scenarios:
    - big file (100Gi): use pararellel composite upload, file is broken to small chunks and uploaded 
    - use application log: use storage class archive
    - logs file you want to access once in quarter: use cold line
    - gcs
    - HA 11(9) annualy
    - alway low latency, undependent storage class

command line:

    $ gsutil ls
    gs://disco-plane-418206.appspot.com/
    gs://disco-plane-418206_cloudbuild/
    gs://gcf-v2-sources-791591819564-asia-southeast1/
    gs://gcf-v2-uploads-791591819564-asia-southeast1/
    gs://hongminhcbg-bucket/
    gs://staging.disco-plane-418206.appspot.com/

    $ gsutil cp gs://staging.disco-plane-418206.appspot.com/1d33de492efb4b04e7e01d181c670c6fc03cdb6f gs://disco-plane-418206.appspot.com/
    $ gsutil rm -r gs://hongminhcbg-bucket/

notes:
 
    - Cloud Storage is serverless and autoscaling
    - Cloud Storage supports partial updates for an object => fail
    - object lifecycle Management:
        detele_to_save_cost:
            conditions_based_on: Age, CreatedBefore, isLive, NumberOfNewerVersions
            two_kind_of_actions:
                - SetStorageClass
                - Deletion
            config: Home -> create bucket -> Add rule

class:
    
    - standard: frequenly
    - nearline: once per month
    - coldline: once per quarter
    - archive: once per year

Notes:

    - cloud container registry use cloud storage as the underlying storage for container images
    - local SSD for hish IOPS workloads
    - control bigquery cost => apply project quota
    - Storage Transfer Service is primarily designed for transferring large amounts of data between different storage services rather than handling streaming data
    - 1000T of data => copy data and ship to Google

Storage Location:

  single_region:
    - lowest latency in same region
    - replication across multiple zone
  dual_region:
    - HA and low latency across 2 regions
    - auto fail over
  multiple_region:
    - highest HA but latency, to serve global user
    - auto failover

Bucket retendtion:

  - set the minimun duration that this bucket's object must be protected from deletion or midification after they uploaded

Storeage:

    - Events(upload, delete) -> Eventarc (native filter) - Cloud func
